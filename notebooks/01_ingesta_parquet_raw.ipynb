{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline: NYC TLC Trips to Postgres RAW Layer\n",
    "\n",
    "## Process Flow\n",
    "1. Download Parquet files locally\n",
    "2. Process with Spark\n",
    "3. Write to Postgres using JDBC\n",
    "\n",
    "## Features\n",
    "- Batch processing with memory optimization\n",
    "- Skip already processed files\n",
    "- Smart download (URL check + streaming)\n",
    "- PyArrow batching for large files\n",
    "- Metadata tracking (run_id, source_year/month, ingested_at_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q psycopg2-binary pandas pyarrow psutil requests python-dotenv\n",
    "print(\"Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "import psutil\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "Run ID: pset4_run_001\n",
      "Date range: 2020-2020\n",
      "Services: ['yellow', 'green']\n",
      "Postgres: postgres:5432/nyc_taxi\n",
      "Local data dir: /home/jovyan/data/parquet\n",
      "Batch size: 5 files\n"
     ]
    }
   ],
   "source": [
    "# Configuration from environment\n",
    "PG_HOST = os.getenv('PG_HOST', 'postgres')\n",
    "PG_PORT = os.getenv('PG_PORT', '5432')\n",
    "PG_DB = os.getenv('PG_DB', 'nyc_taxi')\n",
    "PG_USER = os.getenv('PG_USER', 'taxi_user')\n",
    "PG_PASSWORD = os.getenv('PG_PASSWORD')\n",
    "PG_SCHEMA_RAW = os.getenv('PG_SCHEMA_RAW', 'raw')\n",
    "\n",
    "NYC_TLC_BASE_URL = os.getenv('NYC_TLC_BASE_URL', 'https://d37ci6vzurychx.cloudfront.net/trip-data')\n",
    "START_YEAR = 2020\n",
    "END_YEAR = 2020\n",
    "SERVICES = os.getenv('SERVICES', 'yellow,green').split(',')\n",
    "RUN_ID = os.getenv('RUN_ID', str(uuid.uuid4()))\n",
    "\n",
    "LOCAL_DATA_DIR = \"/home/jovyan/data/parquet\"\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "JDBC_URL = f\"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Date range: {START_YEAR}-{END_YEAR}\")\n",
    "print(f\"Services: {SERVICES}\")\n",
    "print(f\"Postgres: {PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "print(f\"Local data dir: {LOCAL_DATA_DIR}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local directory ready: /home/jovyan/data/parquet\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "print(f\"Local directory ready: {LOCAL_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n",
      "Spark driver memory: 8g\n",
      "Spark executor memory: 6g\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark with Postgres JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC_TLC_Postgres_Ingestion\") \\\n",
    "    .config(\"spark.driver.memory\", os.getenv(\"SPARK_DRIVER_MEMORY\", \"8g\")) \\\n",
    "    .config(\"spark.executor.memory\", os.getenv(\"SPARK_EXECUTOR_MEMORY\", \"6g\")) \\\n",
    "    .config(\"spark.driver.maxResultSize\", os.getenv(\"SPARK_DRIVER_MAXRESULTSIZE\", \"2g\")) \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark driver memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"Spark executor memory: {spark.conf.get('spark.executor.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to process: 24\n"
     ]
    }
   ],
   "source": [
    "def generate_inventory(base_url, services, start_year, end_year):\n",
    "    inventory = []\n",
    "    for service in services:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            for month in range(1, 13):\n",
    "                if year == 2025 and month > datetime.now().month:\n",
    "                    break\n",
    "                \n",
    "                url = f\"{base_url}/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "                local_file = f\"{LOCAL_DATA_DIR}/{service}_{year}_{month:02d}.parquet\"\n",
    "                \n",
    "                inventory.append({\n",
    "                    'service': service,\n",
    "                    'year': year,\n",
    "                    'month': month,\n",
    "                    'url': url,\n",
    "                    'local_file': local_file\n",
    "                })\n",
    "    return inventory\n",
    "\n",
    "inventory = generate_inventory(NYC_TLC_BASE_URL, SERVICES, START_YEAR, END_YEAR)\n",
    "print(f\"Total files to process: {len(inventory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Postgres: PostgreSQL 15.14 on x86_64-pc-linux-musl, compiled...\n"
     ]
    }
   ],
   "source": [
    "def get_postgres_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=PG_HOST,\n",
    "        port=PG_PORT,\n",
    "        database=PG_DB,\n",
    "        user=PG_USER,\n",
    "        password=PG_PASSWORD\n",
    "    )\n",
    "\n",
    "try:\n",
    "    conn = get_postgres_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    version = cursor.fetchone()\n",
    "    print(f\"Connected to Postgres: {version[0][:50]}...\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Postgres connection error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking already processed files in Postgres...\n",
      "  yellow: 12 files already processed\n",
      "  green: 12 files already processed\n",
      "Total files already in Postgres: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking already processed files in Postgres...\")\n",
    "try:\n",
    "    conn = get_postgres_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    existing_run_ids = set()\n",
    "    for service in ['yellow', 'green']:\n",
    "        table_name = f\"{PG_SCHEMA_RAW}.{service}_taxi_trip\"\n",
    "        try:\n",
    "            cursor.execute(f\"SELECT DISTINCT run_id FROM {table_name}\")\n",
    "            results = cursor.fetchall()\n",
    "            service_ids = {row[0] for row in results}\n",
    "            existing_run_ids.update(service_ids)\n",
    "            print(f\"  {service}: {len(service_ids)} files already processed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {service}: table not found or empty (OK for first run)\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(f\"Total files already in Postgres: {len(existing_run_ids)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not check existing files (OK for first run): {e}\")\n",
    "    existing_run_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parquet(url, local_file):\n",
    "    if os.path.exists(local_file):\n",
    "        file_size_mb = os.path.getsize(local_file) / (1024 * 1024)\n",
    "        print(f\"  File exists: {file_size_mb:.2f} MB\")\n",
    "        return True, os.path.getsize(local_file)\n",
    "    \n",
    "    try:\n",
    "        r = requests.head(url, timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"  URL not available: {r.status_code}\")\n",
    "            return False, None\n",
    "    except Exception as e:\n",
    "        print(f\"  URL check failed: {e}\")\n",
    "        return False, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"  Downloading...\")\n",
    "        with requests.get(url, stream=True, timeout=60) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_file, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        file_size = os.path.getsize(local_file)\n",
    "        size_mb = file_size / (1024 * 1024)\n",
    "        print(f\"  Downloaded: {size_mb:.2f} MB\")\n",
    "        return True, file_size\n",
    "    except Exception as e:\n",
    "        print(f\"  Download failed: {e}\")\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_to_postgres(item):\n",
    "    service = item['service']\n",
    "    year = item['year']\n",
    "    month = item['month']\n",
    "    url = item['url']\n",
    "    local_file = item['local_file']\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    status = 'SUCCESS'\n",
    "    error_msg = None\n",
    "    records_loaded = 0\n",
    "    \n",
    "    print(f\"Processing {service} {year}-{month:02d}...\")\n",
    "    \n",
    "    file_run_id = f\"{service}_{year}_{month:02d}\"\n",
    "    if file_run_id in existing_run_ids:\n",
    "        print(f\"  SKIPPING - already in Postgres\")\n",
    "        return {\n",
    "            'run_id': file_run_id,\n",
    "            'service_type': service,\n",
    "            'source_year': year,\n",
    "            'source_month': month,\n",
    "            'record_count': 0,\n",
    "            'started_at': start_time,\n",
    "            'completed_at': datetime.now(),\n",
    "            'status': 'SKIPPED',\n",
    "            'error_message': 'Already processed'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        download_ok, file_size = download_parquet(url, local_file)\n",
    "        if not download_ok:\n",
    "            raise Exception(\"Download failed or URL not available\")\n",
    "        \n",
    "        df = spark.read.parquet(local_file)\n",
    "\n",
    "        if 'ehail_fee' in df.columns:\n",
    "            df = df.drop('ehail_fee')\n",
    "        \n",
    "        df = df.withColumn(\"run_id\", lit(file_run_id)) \\\n",
    "               .withColumn(\"service_type\", lit(service)) \\\n",
    "               .withColumn(\"source_year\", lit(year)) \\\n",
    "               .withColumn(\"source_month\", lit(month)) \\\n",
    "               .withColumn(\"ingested_at_utc\", current_timestamp()) \\\n",
    "               .withColumn(\"source_path\", lit(url))\n",
    "        \n",
    "        records_loaded = df.count()\n",
    "        print(f\"  Rows: {records_loaded:,}\")\n",
    "        \n",
    "        table_name = f\"{PG_SCHEMA_RAW}.{service}_taxi_trip\"\n",
    "        \n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", JDBC_URL) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", PG_USER) \\\n",
    "            .option(\"password\", PG_PASSWORD) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(f\"  Written to {table_name}\")\n",
    "        \n",
    "        try:\n",
    "            os.remove(local_file)\n",
    "            print(f\"  Temp file cleaned\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    except Exception as e:\n",
    "        status = 'FAILED'\n",
    "        error_msg = str(e)\n",
    "        print(f\"  FAILED: {error_msg}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    print(f\"  Duration: {duration:.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        'run_id': file_run_id,\n",
    "        'service_type': service,\n",
    "        'source_year': year,\n",
    "        'source_month': month,\n",
    "        'record_count': records_loaded,\n",
    "        'started_at': start_time,\n",
    "        'completed_at': end_time,\n",
    "        'status': status,\n",
    "        'error_message': error_msg,\n",
    "        'source_path': url\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Single file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: yellow 2020-01\n",
      "\n",
      "Processing yellow 2020-01...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Result: SKIPPED\n",
      "Status: Already processed\n"
     ]
    }
   ],
   "source": [
    "test_item = inventory[0]\n",
    "print(f\"Testing: {test_item['service']} {test_item['year']}-{test_item['month']:02d}\")\n",
    "print()\n",
    "\n",
    "test_result = ingest_to_postgres(test_item)\n",
    "print()\n",
    "print(f\"Result: {test_result['status']}\")\n",
    "if test_result['status'] == 'SUCCESS':\n",
    "    print(f\"Records loaded: {test_result['record_count']:,}\")\n",
    "    print(\"Ready for batch processing\")\n",
    "else:\n",
    "    print(f\"Status: {test_result['error_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute batch ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch ingestion of 24 files\n",
      "Batch size: 5 files at a time\n",
      "\n",
      "Processing yellow 2020-01...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-02...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-03...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-04...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-05...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "--- Completed 5/24 files ---\n",
      "\n",
      "Processing yellow 2020-06...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-07...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-08...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-09...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-10...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "--- Completed 10/24 files ---\n",
      "\n",
      "Processing yellow 2020-11...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing yellow 2020-12...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-01...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-02...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-03...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "--- Completed 15/24 files ---\n",
      "\n",
      "Processing green 2020-04...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-05...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-06...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-07...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-08...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "--- Completed 20/24 files ---\n",
      "\n",
      "Processing green 2020-09...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-10...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-11...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Processing green 2020-12...\n",
      "  SKIPPING - already in Postgres\n",
      "\n",
      "Ingestion complete: 24 files processed\n"
     ]
    }
   ],
   "source": [
    "audit_records = []\n",
    "\n",
    "print(f\"Starting batch ingestion of {len(inventory)} files\")\n",
    "print(f\"Batch size: {BATCH_SIZE} files at a time\\n\")\n",
    "\n",
    "for i, item in enumerate(inventory, 1):\n",
    "    audit_record = ingest_to_postgres(item)\n",
    "    audit_records.append(audit_record)\n",
    "    print(\"\")\n",
    "    \n",
    "    if i % BATCH_SIZE == 0:\n",
    "        print(f\"--- Completed {i}/{len(inventory)} files ---\\n\")\n",
    "        spark.catalog.clearCache()\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"Ingestion complete: {len(audit_records)} files processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write audit records to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting audit: 'source_path'\n",
      "Error inserting audit: 'source_path'\n",
      "Error inserting audit: 'source_path'\n",
      "Error inserting audit: 'source_path'\n",
      "Error inserting audit: 'source_path'\n",
      "Audit records inserted: 0, Failed: 24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "audit_df = pd.DataFrame(audit_records)\n",
    "\n",
    "conn = get_postgres_connection()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "inserted = 0\n",
    "failed = 0\n",
    "\n",
    "for index, row in audit_df.iterrows():\n",
    "    try:\n",
    "        started_at = row['started_at'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        completed_at = row['completed_at'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        duration = (row['completed_at'] - row['started_at']).total_seconds()\n",
    "        \n",
    "        if pd.isna(row['error_message']) or not row['error_message']:\n",
    "            error_msg = 'NULL'\n",
    "        else:\n",
    "            error_msg = \"'\" + str(row['error_message']).replace(\"'\", \"''\") + \"'\"\n",
    "        \n",
    "        source_path = f\"'{row['source_path']}'\"\n",
    "        \n",
    "        sql = f\"\"\"\n",
    "        INSERT INTO {PG_SCHEMA_RAW}.ingestion_audit \n",
    "        (run_id, service_type, source_year, source_month, source_path, record_count, \n",
    "         status, error_message, started_at, completed_at, duration_seconds)\n",
    "        VALUES (\n",
    "            '{row['run_id']}',\n",
    "            '{row['service_type']}',\n",
    "            {row['source_year']},\n",
    "            {row['source_month']},\n",
    "            {source_path},\n",
    "            {row['record_count']},\n",
    "            '{row['status']}',\n",
    "            {error_msg},\n",
    "            '{started_at}',\n",
    "            '{completed_at}',\n",
    "            {duration}\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(sql)\n",
    "        inserted += 1\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        if failed <= 5:\n",
    "            print(f\"Error inserting audit: {str(e)[:80]}\")\n",
    "    \n",
    "    if (index + 1) % 50 == 0:\n",
    "        print(f\"Progress: {index + 1}/{len(audit_df)}\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Audit records inserted: {inserted}, Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INGESTION SUMMARY ===\n",
      "\n",
      "Total files processed: 24\n",
      "\n",
      "By status:\n",
      "status\n",
      "SKIPPED    24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "By service and status:\n",
      "service_type  status \n",
      "green         SKIPPED    12\n",
      "yellow        SKIPPED    12\n",
      "dtype: int64\n",
      "\n",
      "✓ No failed loads\n"
     ]
    }
   ],
   "source": [
    "summary_df = pd.DataFrame(audit_records)\n",
    "\n",
    "print(\"\\n=== INGESTION SUMMARY ===\")\n",
    "print(f\"\\nTotal files processed: {len(summary_df)}\")\n",
    "print(f\"\\nBy status:\")\n",
    "print(summary_df['status'].value_counts())\n",
    "\n",
    "print(f\"\\nBy service and status:\")\n",
    "print(summary_df.groupby(['service_type', 'status']).size())\n",
    "\n",
    "successful = summary_df[summary_df['status'] == 'SUCCESS']\n",
    "if len(successful) > 0:\n",
    "    total_records = successful['record_count'].sum()\n",
    "    print(f\"\\nTotal records ingested: {total_records:,}\")\n",
    "\n",
    "failed = summary_df[summary_df['status'] == 'FAILED']\n",
    "if len(failed) > 0:\n",
    "    print(f\"\\nFailed loads:\")\n",
    "    print(failed[['service_type', 'source_year', 'source_month', 'error_message']])\n",
    "else:\n",
    "    print(f\"\\n✓ No failed loads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
